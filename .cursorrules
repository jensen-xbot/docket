# Project: Docket

## Overview
Simple, fast iPhone app for managing your daily tasks without the noise.
Voice-to-task with conversational AI lets users create, categorize, annotate, and share tasks by speaking naturally.

## Project Status
- **MVP (Phases 1-4):** Complete — SwiftData, CRUD, categories, UI polish
- **v1.0 (Phase 5):** Complete — Supabase cloud sync, auth, offline queue, sharing
- **v1.1 (Phases 6-9):** In progress — Conversational voice-to-task

## Tech Stack
- **Frontend:** SwiftUI (iOS 17+)
- **Backend:** Supabase (PostgreSQL + Auth + Edge Functions)
- **Local persistence:** SwiftData
- **Cloud sync:** Supabase bi-directional with offline queue
- **Voice transcription:** Apple SFSpeechRecognizer (on-device, free)
- **TTS readback:** AVSpeechSynthesizer (on-device, free)
- **AI parsing:** gpt-4.1-mini via OpenRouter → Supabase Edge Function
- **Audio capture:** AVAudioEngine + AVAudioSession
- **No orchestrator:** Single-shot/conversational API calls — no LangChain/LangGraph

## Architecture Patterns
- MVVM (Model-View-ViewModel) for SwiftUI
- Repository pattern for data layer abstraction
- SwiftData for local persistence
- Supabase client for remote sync
- Conversational voice loop: listen → transcribe → AI parse → TTS readback → confirm
- Edge Function as secure AI proxy (API keys never on client)

## Conventions
- **Naming:** Swift style — PascalCase types, camelCase variables/functions
- **File structure:** Feature-based grouping (Tasks/, Auth/, Common/)
- **State management:** @Observable ViewModels, injected as environment objects
- **Access control:** Use `private` and `fileprivate` appropriately
- **Swift version:** 6.0+ with strict concurrency checking

## Code Quality
- **Error Handling:** Use `throws` for recoverable errors, `Result` for async operations, never force unwrap
- **SwiftData:** All models must conform to `@Model`, use `@Attribute(.unique)` for IDs
- **Previews:** Every view must have a `#Preview` with sample data
- **Comments:** Document public APIs only; code should be self-documenting

## UI/UX Guidelines
- **Quick capture:** 2-3 taps max to create a task
- **Visual hierarchy:** Use color coding for priority and urgency
  - **Priority:** Low (gray) → Medium (orange) → High (red)
  - **Due dates:** Far future (green) → Within 3 days (yellow) → Overdue/past (red)
- **Animations:** Subtle, native-feeling transitions (0.2-0.3s)
- **Empty states:** Friendly copy, clear call-to-action
- **Dark mode:** Support both light and dark from day 1
- **Accessibility:** Minimum 44pt tap targets, Dynamic Type support

## Data Model
```swift
@Model
class Task {
    @Attribute(.unique) var id: UUID
    var title: String
    var isCompleted: Bool
    var createdAt: Date
    var dueDate: Date?
    var priority: Priority // low, medium, high
    var category: String?
    var notes: String?
    var completedAt: Date?
}

enum Priority: Int, CaseIterable {
    case low = 0      // gray
    case medium = 1   // orange  
    case high = 2     // red
}

// Voice-to-task AI response (not persisted — intermediate struct)
struct ParsedTask: Codable, Identifiable {
    let id: UUID
    var title: String
    var dueDate: Date?
    var priority: String   // "low", "medium", "high"
    var category: String?
    var notes: String?
    var shareWith: String? // email or display name
    var suggestion: String?
}

struct ConversationMessage: Codable {
    let role: String       // "user" or "assistant"
    let content: String
}

struct ParseResponse: Codable {
    let type: String       // "question" or "complete"
    let text: String?      // follow-up question (when type == "question")
    let tasks: [ParsedTask]? // extracted tasks (when type == "complete")
    let summary: String?   // TTS readback text (when type == "complete")
}
```

## Key Files

### Core
- `DocketApp.swift` — App entry point with SwiftData container setup
- `Models/Task.swift` — Task model with helper properties
- `Views/TaskListView.swift` — Main task list with filtering
- `Views/TaskRowView.swift` — Individual task cell
- `Views/AddTaskView.swift` — Create task sheet
- `Views/EditTaskView.swift` — Edit existing task
- `ViewModels/TaskListViewModel.swift` — Business logic

### Cloud Sync (v1.0)
- `Managers/SyncEngine.swift` — Bi-directional Supabase sync with offline queue
- `Managers/NetworkMonitor.swift` — NWPathMonitor connectivity detection

### Voice-to-Task (v1.1 — in progress)
- `Managers/SpeechRecognitionManager.swift` — SFSpeechRecognizer + AVAudioEngine wrapper
- `Managers/TTSManager.swift` — AVSpeechSynthesizer wrapper for TTS readback
- `Managers/VoiceTaskParser.swift` — Calls Supabase Edge Function for AI parsing
- `Views/VoiceRecordingView.swift` — Mic button + conversation overlay
- `Views/TaskConfirmationView.swift` — Parsed task list with edit/confirm
- `Models/ParsedTask.swift` — Lightweight struct for AI response

### Utilities
- `Utilities/Date+Extensions.swift` — Due date formatting and color logic
- `Utilities/Color+Extensions.swift` — Priority/urgency color helpers

## Voice-to-Task Guidelines (v1.1)
- **Conversational AI:** The voice flow is a multi-turn conversation, not single-shot
- **Audio session management:** Must switch between `.record` (mic) and `.playback` (TTS) cleanly
  - Use `.playAndRecord` category with `.defaultToSpeaker` override when possible
  - Handle `AVAudioSession.interruptionNotification` for phone calls, Siri, etc.
  - After TTS finishes (`speechSynthesizer(_:didFinish:)`), re-activate mic
- **Conversation state:** Managed client-side as a `[(role: String, content: String)]` messages array
- **Edge Function:** Receives messages array, returns either a follow-up question or completed tasks
- **TTS:** Uses AI-generated `summary` field for natural readback — not raw field values
- **Share resolution:** `shareWith` name → contacts cache → email → `task_shares` table
- **Permissions required:** Microphone (`NSMicrophoneUsageDescription`), Speech Recognition (`NSSpeechRecognitionUsageDescription`)
- **No new dependencies:** Speech, AVFoundation, AVSpeechSynthesizer are all built into iOS
- See [VOICE-TO-TASK-V2.md](VOICE-TO-TASK-V2.md) for full architecture

## Development Workflow
- Build iteratively: Model → ViewModel → View → Preview → Test on device
- Always test dark mode and light mode
- Check dynamic type (accessibility sizes) on each view
- Use SF Symbols for icons (consistent with iOS)
- Prefer native SwiftUI components over custom views

## Build Verification (REQUIRED)
⚠️ **After ANY code changes, ALWAYS run a build verification before presenting results to the user:**

```bash
cd /Users/johnyblueyes/Documents/Docket/docket/Docket && xcodebuild -project Docket.xcodeproj -scheme Docket -destination 'platform=iOS Simulator,name=iPhone 17 Pro' build 2>&1 | tail -30
```

**Checklist before declaring done:**
1. Run `xcodebuild` and confirm `** BUILD SUCCEEDED **`
2. If build fails, fix ALL errors and re-run until it passes
3. When creating new `.swift` files, you MUST add them to `project.pbxproj`:
   - Add a `PBXFileReference` entry
   - Add a `PBXBuildFile` entry
   - Add the file ref to the correct `PBXGroup` (Models/, Views/, Utilities/, Managers/)
   - Add the build file to the `PBXSourcesBuildPhase`
4. Never present code changes as "done" without a passing build

## Testing Strategy
- Unit tests for ViewModels (business logic)
- UI tests for critical flows (create → complete → delete)
- Manual testing: light/dark mode, rotation, accessibility sizes

## Context
This app is for Jon — a busy professional juggling sales work, side projects (Closelo), family, and personal routines. Needs quick capture (2-3 taps) and clear organization without friction.

## Development Notes
- iOS 17+ target (latest SwiftUI features like `@Observable` macro)
- Dark mode from day 1 (SwiftUI handles this via `@Environment(\.colorScheme)`)
- MVP complete — local persistence with SwiftData
- v1.0 complete — cloud sync via Supabase with offline queue + conflict resolution
- v1.1 in progress — conversational voice-to-task with multi-turn dialogue (see VOICE-TO-TASK-V2.md)
- AI model: gpt-4.1-mini via OpenRouter (fast, cheap, structured output)
- No thinking models (o1/o3/R1) — task parsing is pattern extraction, not reasoning
- No orchestrator (LangChain/LangGraph) — conversation loop is ~15 lines of Swift on the client

## Critical iOS Project Setup
⚠️ **MUST HAVE in project.pbxproj buildSettings:**
```
INFOPLIST_KEY_UILaunchScreen_Generation = YES;  // Prevents letterbox/black bars
INFOPLIST_KEY_UISupportedInterfaceOrientations = UIInterfaceOrientationPortrait;
INFOPLIST_KEY_CFBundleDisplayName = Docket;
INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;  // Requires AppIcon in Assets
```
Without `UILaunchScreen_Generation`, iOS runs app in legacy compatibility mode with black bars (looks like iPhone 4). See IOS-GOTCHAS.md for details.

**Required Files:**
- `Assets.xcassets/AppIcon.appiconset/Contents.json` (empty icon set satisfies build)
- Real 1024x1024 icon can be added later

**After build settings changes:**
1. Delete app from simulator/device
2. Clean Build Folder (Shift+Cmd+K)
3. Run again (Cmd+R)
