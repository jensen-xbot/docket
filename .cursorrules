# Project: Docket

## Overview
Simple, fast iPhone app for managing your daily tasks without the noise.
Voice-to-task with conversational AI lets users create, categorize, annotate, and share tasks by speaking naturally.

## Project Status
- **MVP (Phases 1-4):** Complete — SwiftData, CRUD, categories, UI polish
- **v1.0 (Phase 5):** Complete — Supabase cloud sync, auth, offline queue, sharing
- **v1.1 (Phases 6-9):** In progress — Conversational voice-to-task
- **Phase 13 (Unified AI Command Bar):** Upcoming — bottom command bar, voice + text unified, search implicit. See [UNIFIED-AI-COMMAND-BAR.md](UNIFIED-AI-COMMAND-BAR.md)

## Tech Stack
- **Frontend:** SwiftUI (iOS 17+)
- **Backend:** Supabase (PostgreSQL + Auth + Edge Functions)
- **Local persistence:** SwiftData
- **Cloud sync:** Supabase bi-directional with offline queue
- **Voice transcription:** Apple SFSpeechRecognizer (on-device, free)
- **TTS readback:** OpenAI gpt-4o-mini-tts (streaming PCM) / AVSpeechSynthesizer (fallback)
- **AI parsing:** gpt-4.1-mini via OpenRouter → Supabase Edge Function
- **Audio capture:** AVAudioEngine + AVAudioSession
- **No orchestrator:** Single-shot/conversational API calls — no LangChain/LangGraph

## Architecture Patterns
- MVVM (Model-View-ViewModel) for SwiftUI
- Repository pattern for data layer abstraction
- SwiftData for local persistence
- Supabase client for remote sync
- Conversational voice loop: listen → transcribe → AI parse → TTS readback → confirm
- Edge Function as secure AI proxy (API keys never on client)

## Conventions
- **Naming:** Swift style — PascalCase types, camelCase variables/functions
- **File structure:** Feature-based grouping (Tasks/, Auth/, Common/)
- **State management:** @Observable ViewModels, injected as environment objects
- **Access control:** Use `private` and `fileprivate` appropriately
- **Swift version:** 6.0+ with strict concurrency checking

## Code Quality
- **Error Handling:** Use `throws` for recoverable errors, `Result` for async operations, never force unwrap
- **SwiftData:** All models must conform to `@Model`, use `@Attribute(.unique)` for IDs
- **Previews:** Every view must have a `#Preview` with sample data
- **Comments:** Document public APIs only; code should be self-documenting

## UI/UX Guidelines
- **Quick capture:** 2-3 taps max to create a task
- **Visual hierarchy:** Use color coding for priority and urgency
  - **Priority:** Low (gray) → Medium (orange) → High (red)
  - **Due dates:** Far future (green) → Within 3 days (yellow) → Overdue/past (red)
- **Animations:** Subtle, native-feeling transitions (0.2-0.3s)
- **Empty states:** Friendly copy, clear call-to-action
- **Dark mode:** Support both light and dark from day 1
- **Accessibility:** Minimum 44pt tap targets, Dynamic Type support

## Data Model
```swift
@Model
class Task {
    @Attribute(.unique) var id: UUID
    var title: String
    var isCompleted: Bool
    var createdAt: Date
    var dueDate: Date?
    var priority: Priority // low, medium, high
    var category: String?
    var notes: String?
    var completedAt: Date?
}

enum Priority: Int, CaseIterable {
    case low = 0      // gray
    case medium = 1   // orange  
    case high = 2     // red
}

// Voice-to-task AI response (not persisted — intermediate struct)
struct ParsedTask: Codable, Identifiable {
    let id: UUID
    var title: String
    var dueDate: Date?
    var hasTime: Bool      // true if AI returned HH:mm, false if date-only
    var priority: String   // "low", "medium", "high"
    var category: String?
    var notes: String?
    var shareWith: String? // email or display name
    var suggestion: String?
    var checklistItems: [String]?  // AI-suggested item names (ad-hoc grocery list)
    var useTemplate: String?       // store name whose template to load
}

struct ConversationMessage: Codable {
    let role: String       // "user" or "assistant"
    let content: String
}

struct ParseResponse: Codable {
    let type: String       // "question", "complete", "update", or "delete"
    let text: String?      // follow-up question (when type == "question")
    let tasks: [ParsedTask]? // extracted tasks (when type == "complete")
    let taskId: String?    // existing task ID (when type == "update" or "delete")
    let changes: TaskChanges? // fields to change (when type == "update")
    let summary: String?   // TTS readback (when type == "complete", "update", or "delete")
}
```

## Key Files

### Core
- `DocketApp.swift` — App entry point with SwiftData container setup
- `Models/Task.swift` — Task model with helper properties
- `Views/TaskListView.swift` — Main task list with filtering
- `Views/TaskRowView.swift` — Individual task cell
- `Views/AddTaskView.swift` — Create task sheet
- `Views/EditTaskView.swift` — Edit existing task
- `ViewModels/TaskListViewModel.swift` — Business logic

### Cloud Sync (v1.0)
- `Managers/SyncEngine.swift` — Bi-directional Supabase sync with offline queue
- `Managers/NetworkMonitor.swift` — NWPathMonitor connectivity detection

### Voice-to-Task (v1.1 — in progress)
- `Managers/SpeechRecognitionManager.swift` — SFSpeechRecognizer + AVAudioEngine wrapper
- `Managers/TTSManager.swift` — OpenAI TTS (primary) + AVSpeechSynthesizer (fallback) for readback
- `Managers/VoiceTaskParser.swift` — Calls Supabase Edge Function for AI parsing
- `Views/VoiceRecordingView.swift` — Mic button + conversation overlay (to be absorbed into CommandBar per Phase 13)
- `Views/TaskConfirmationView.swift` — Parsed task list with edit/confirm
- `Models/ParsedTask.swift` — Lightweight struct for AI response (includes TaskContext, TaskChanges)

### Unified AI Command Bar (Phase 13 — planned)
- `Views/CommandBarView.swift` — Bottom command bar, voice + text unified, expansion animation
- `Views/ConversationView.swift` — Shared chat UI (extracted from VoiceRecordingView)
- `Views/TextInputBar.swift` — Compose bar + mode switch (text/voice/+)
- See [UNIFIED-AI-COMMAND-BAR.md](UNIFIED-AI-COMMAND-BAR.md)

### Sharing & Notifications (v1.1)
- `Views/ShareTaskView.swift` — Share method sheet (Docket-first primary action)
- `Views/ContactsListView.swift` — Contact management + pending invite accept/decline
- `Views/NotificationsListView.swift` — Bell badge + notification inbox
- `Views/SharedAvatarView.swift` — Dual-avatar with tap-to-reveal "Shared by/with" label

### Utilities
- `Utilities/Date+Extensions.swift` — Due date formatting and color logic
- `Utilities/Color+Extensions.swift` — Priority/urgency color helpers

## Voice-to-Task Guidelines (v1.1)
- **Conversational AI:** The voice flow is a multi-turn conversation, not single-shot
- **Audio session management:** Must switch between `.record` (mic) and `.playback` (TTS) cleanly
  - Use `.playAndRecord` category with `.defaultToSpeaker` override when possible
  - Handle `AVAudioSession.interruptionNotification` for phone calls, Siri, etc.
  - After TTS finishes (`speechSynthesizer(_:didFinish:)`), re-activate mic
- **Conversation state:** Managed client-side as a `[(role: String, content: String)]` messages array
- **Edge Function:** Receives messages array, returns either a follow-up question or completed tasks
- **TTS:** OpenAI TTS API (natural voices, nova default) with Apple AVSpeechSynthesizer fallback
- **Share resolution:** `shareWith` name → contacts cache → email → `task_shares` table
- **Permissions required:** Microphone (`NSMicrophoneUsageDescription`), Speech Recognition (`NSSpeechRecognitionUsageDescription`)
- **No new dependencies:** Speech, AVFoundation, AVSpeechSynthesizer are all built into iOS
- See [VOICE-TO-TASK-V2.md](VOICE-TO-TASK-V2.md) for full architecture
- See [UNIFIED-AI-COMMAND-BAR.md](UNIFIED-AI-COMMAND-BAR.md) for Unified AI Command Bar (voice + text unified, Phase 13)

## Voice UX Patterns (Learned the Hard Way)
- **View identity = stability:** Live transcription and committed messages MUST share the same `.id()` scheme. Use a unified `displayMessages` computed property where the live text gets `id: "msg-\(messages.count)"` — the same ID it will have once committed. Prevents flicker.
- **Guard stale speech callbacks:** `SFSpeechRecognizer` delivers one final callback after `endAudio()`/`cancel()`. Always `guard manager.isRecording else { return }` in the recognition handler.
- **phaseAnimator > .animation(.repeatForever):** `.animation(.repeatForever, value:)` doesn't restart on re-entry. Use `phaseAnimator([false, true], trigger: state)` for repeating animations that reliably restart every time the mic turns on.
- **Don't withAnimation scroll calls:** `withAnimation { scrollProxy.scrollTo() }` animates content insertions, not just scroll. Use `DispatchQueue.main.async { scrollProxy.scrollTo() }` instead.
- **Audio level feedback:** Calculate RMS in the audio tap (nonisolated), poll to @MainActor at ~12fps with EMA smoothing (0.3 old + 0.7 new). Mask a colored mic icon from bottom with the level value.
- **Bottom-anchored chat:** Use `GeometryReader` + `.frame(minHeight: geometry.size.height, alignment: .bottom)` inside ScrollView so messages anchor to the bottom when few.

## Voice-First Feature Development (MANDATORY)
Every feature that creates, modifies, queries, or deletes data MUST be exposed to the voice-task AI. Voice is not an afterthought — it's a first-class input method alongside the GUI.

**When adding any new feature, you MUST:**

1. **Update the Edge Function prompt** (`parse-voice-tasks/index.ts`):
   - Add the new capability to the system prompt so the AI knows it exists
   - Define what voice utterances should trigger it (e.g., "set progress to 50%", "assign this to Sarah")
   - Add the new fields/actions to the response schema

2. **Extend ParsedTask / TaskContext / TaskChanges** (`Models/ParsedTask.swift`):
   - New fields on task creation → add to `ParsedTask`
   - New queryable state → add to `TaskContext` (sent as context each turn)
   - New modifiable fields → add to `TaskChanges` (for voice updates)

3. **Wire up CommandBar / conversation view handlers** (Phase 13: VoiceRecordingView absorbed into CommandBar):
   - New response types or new fields in `type: "complete"` → handle in `saveTasks()`
   - New fields in `type: "update"` → handle in the update handler
   - New intents (e.g., "send to Slack") → add to IntentClassifier if local, or Edge Function if semantic

4. **Test voice path**:
   - Verify the AI can parse natural utterances for the new feature
   - Verify the iOS side correctly handles the new fields/actions
   - Verify TTS readback mentions the new capability naturally

**Examples of features that required voice surface:**
- Recurring tasks → `recurrenceRule` in ParsedTask + Edge Function prompt + saveTasks handler
- Task updates/deletes → `type: "update"/"delete"` + TaskContext + TaskChanges + update/delete handlers
- Grocery templates → `checklistItems`/`useTemplate` in ParsedTask + store context in prompt
- Progress tracking → `progressPercentage` in TaskChanges + voice commands in prompt
- Sharing → `shareWith` in ParsedTask + share resolution flow
- Integrations → "send to Slack channel" in prompt + integration-aware context

**If a feature cannot be voice-controlled, document WHY in the task spec** (e.g., "OAuth setup requires browser redirect — GUI only, but triggering a send via voice is supported after setup").

## Personalization Adaptation Guidelines (v1.2)
- **Learn from explicit edits only:** do not infer personalized mappings without a user correction signal.
- **Scope by source:** prioritize corrections on voice-created tasks to avoid polluting signal from unrelated manual edits.
- **Keep profile compact:** store ranked mappings (recency + frequency), cap each list, and prune stale entries.
- **Privacy first:** no raw audio retention for personalization; no transcript payloads in analytics.
- **User control required:** include opt-in/opt-out and "reset learned voice data" in Profile.
- **Prompt discipline:** inject only relevant personalization snippets (top N), never full correction history.

## Supabase Edge Function Deployment
⚠️ **All Docket Edge Functions are deployed with `--no-verify-jwt`:**
```bash
supabase functions deploy <function-name> --no-verify-jwt
```
- The Supabase API gateway's built-in JWT check is **disabled** for our functions.
- Each function handles its own auth internally (checks `Authorization` header, calls `supabase.auth.getUser()`).
- If you deploy **without** `--no-verify-jwt`, the gateway will reject requests with 401 before your function code runs, even if the client sends a valid token.
- This applies to: `text-to-speech`, `parse-voice-tasks`, `transcribe-audio`, `push-share-notification`.

## Development Workflow
- Build iteratively: Model → ViewModel → View → Preview → Test on device
- Always test dark mode and light mode
- Check dynamic type (accessibility sizes) on each view
- Use SF Symbols for icons (consistent with iOS)
- Prefer native SwiftUI components over custom views

## Build Verification (REQUIRED)
⚠️ **After ANY code changes, ALWAYS run a build verification before presenting results to the user:**

```bash
cd /Users/johnyblueyes/Documents/Docket/docket/Docket && xcodebuild -project Docket.xcodeproj -scheme Docket -destination 'platform=iOS Simulator,name=iPhone 17 Pro' build 2>&1 | tail -30
```

**Checklist before declaring done:**
1. Run `xcodebuild` and confirm `** BUILD SUCCEEDED **`
2. If build fails, fix ALL errors and re-run until it passes
3. When creating new `.swift` files, you MUST add them to `project.pbxproj`:
   - Add a `PBXFileReference` entry
   - Add a `PBXBuildFile` entry
   - Add the file ref to the correct `PBXGroup` (Models/, Views/, Utilities/, Managers/)
   - Add the build file to the `PBXSourcesBuildPhase`
4. Never present code changes as "done" without a passing build

## Testing Strategy
- Unit tests for ViewModels (business logic)
- UI tests for critical flows (create → complete → delete)
- Manual testing: light/dark mode, rotation, accessibility sizes

## Context
This app is for Jon — a busy professional juggling sales work, side projects (Closelo), family, and personal routines. Needs quick capture (2-3 taps) and clear organization without friction.

## Development Notes
- iOS 17+ target (latest SwiftUI features like `@Observable` macro)
- Dark mode from day 1 (SwiftUI handles this via `@Environment(\.colorScheme)`)
- MVP complete — local persistence with SwiftData
- v1.0 complete — cloud sync via Supabase with offline queue + conflict resolution
- v1.1 in progress — conversational voice-to-task with multi-turn dialogue (see VOICE-TO-TASK-V2.md)
- AI model: gpt-4.1-mini via OpenRouter (fast, cheap, structured output)
- No thinking models (o1/o3/R1) — task parsing is pattern extraction, not reasoning
- No orchestrator (LangChain/LangGraph) — conversation loop is ~15 lines of Swift on the client

## Critical iOS Project Setup
⚠️ **MUST HAVE in project.pbxproj buildSettings:**
```
INFOPLIST_KEY_UILaunchScreen_Generation = YES;  // Prevents letterbox/black bars
INFOPLIST_KEY_UISupportedInterfaceOrientations = UIInterfaceOrientationPortrait;
INFOPLIST_KEY_CFBundleDisplayName = Docket;
INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;  // Requires AppIcon in Assets
```
Without `UILaunchScreen_Generation`, iOS runs app in legacy compatibility mode with black bars (looks like iPhone 4). See IOS-GOTCHAS.md for details.

**Required Files:**
- `Assets.xcassets/AppIcon.appiconset/Contents.json` (empty icon set satisfies build)
- Real 1024x1024 icon can be added later

**After build settings changes:**
1. Delete app from simulator/device
2. Clean Build Folder (Shift+Cmd+K)
3. Run again (Cmd+R)
